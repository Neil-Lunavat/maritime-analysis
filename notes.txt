Data Science Project 

Data Gathering
marrinetraffic.com 
used the network tab in the inspect window to find where the website sends get request to the backend 
found that it sends get request to https://www.marinetraffic.com/getData/get_data_json_4/z:0/X:0/Y:0/station:0
therefore I sent it as well and got the json data 
https://www.marinetraffic.com/getData/get_data_json_4/z:1/X:0/Y:0/station:0
After exploring a little bit, I found that z is the zoom value 
when z = 0 we're zoomed out completely and only have x=0 y=0 value accessible (still 2.5k entries)
so z=1 and permutating over x and y values with 00 01 10 and 11 values we have the 4 json files
Each request gives up to 2.5 thousand entries. Combined scraping gets us around 10 thousand entries
this is good enough data to analyze and hopefully do some machine learning on
unfortunately while trying to configure this script my IP got blocked. 

converted it to CSV using pandas using script.py

Goals
An optimistic outlook on opportunities (before analysis of if it's really possible, ideas on what we can do with the data):
VIS
-> port traffic analysis 
-> heatmaps of vessel density and traffic patterns 
ML
-> Vessel Type Classification based on movement patterns
-> Anomaly detection (unusual speed patterns, eg. cargo ship moving at 200 knots)
-> Suspicious movement patterns (would require continuous data overtime)

features that would help: 
LAT LON SPEED COURSE 
SHIPTYPE STATUS_NAME TYPE_NAME
Vessel Dimensions 


Data Exploration
firstly, the usage of AI to speed up the process of coding without worrying of syntax was a lot in this project
in terms of data exploration, the first objective was to understand what each column meant and how they relate to each other 
understanding the coverage of data in terms of how many NaN values vs valid values
if it was possible to fill up NaN values using the other data 
seeing which columns could be dropped that pose no significant value (feature engineering)
to explore we first use some general functions of pandas like .info .describe etc
from this we understand a little about the expanse of ships around the maps
latitude's range being -67 to 81 degrees (basically from antarctica to north, as low and as high as the ships can go)
longitude's range from -179 to 179 (covers the map)
both are perfect, cover the entire map, we can say we have scraped all the ship data of one particular moment in time
Remark: the use of common sense and general knowledge is greatly helpful in data analytics
if it's financial analysis of a business, knowing about that business would help!

An acute observation of speed is when I scraped in the morning the max speed was 1200 knots, and now it's 405 knots
basically ships slow down at night because of the tides! 

ROT is just rate of turning, it can be filled up with 0 for ships with a lot of speed since they won't be turning at all 
(they would be on course)
SHIPTYPE and TYPE_IMG have a lot of similarities, maybe they indicate the same thing
but SHIPTYPE has more coverage, if they're always equal, we can use SHIPTYPE to infer TYPE_NAME
this will increase the coverage of the categorized column, helpful in classification problems

SPEED's histogram gives a uniform distribution with two peaks, one around 0-20 range and the other at 120 range
this means a lot of ships stationary are being counted, we have a diverse dataset without biases towards one type
dropping the stationary ships with speed 0 and overlaying the graph on the prior histogram, my prediction was right
the peak at 0-20 range got smaller but still exists, maybe they are yachts or something 

Pleasure Craft have the highest record speeds of 405 knots

There's no correlation with boat length and speed, all three categories (<50m, 50-150m, >150m) have equivalent mean speeds
however larger boats are generally 50 knots faster

HEADING vs COURSE
heading is where the front of ship points
course is where the ship is moving towards
can happen that ship is backwards moving due to waves (which is problematic)
we can calculate max angular difference between them (very useful) (higher value = more out of alignment = problem) 
for low / no speed the angular difference doesn't matter as much 
for high speeds (10 - 1000) the distribution is normal except for outliers (0 25 50 and 75 are all < 5 but max is 180)
this 180 outlier could be possible that satellite mistook the ship's head for its tail
but higher angular differences for higher speeds = wind and waves are blowing ships away 
plotting speed v/s length and marking the angle difference greater than 90 as red triangles
we see that there's no true correlation between "danger" that we suspected and speed OR length
all ships are subject to this "swaying" from the wind / waves therfore there's no valuable classification to be done 

plotting ROT doesn't give any useful insight either, we just understand that there are a lot of ships that are currently 
not rotating therfore we can replace the NaN values with 0 there (given HC_DIFF_ABS is < some logical threshold)
and speed is > than some other threshold
the reason is if the angular difference is greater, it means the ship is not on its course
and if speed is less it means they're just "thinking" of course correction or something 
ALTHOUGH to save computational costs if this data is being used in the real world, we can see through the ROT v/s speed plot
that MOST of the NaN values for ROT are at high speeds or near 0 speeds. (ship has stopped / moving on correct course)
both of which do not need rotation, therefore we can replace ALL NaN values with 0 to make it a usable feature in our dataset

a simple linear regression can fit the width to length plot 

some width values were 0 for some reason.
TODO: find the reason why some width values were 0, they should've been NaN atleast? 

key thing I'm doing is AI had given TYPE_NAME against the width but all entries that have TYPE_NAME don't have width... 
Instead I'm using SHIPTYPE since it's ACTUALLY the same thing as we saw previously, therefore using the correlation between
TYPE_IMG and TYPE_NAME given to us, and TYPE_IMG == SHIPTYPE, and SHIPTYPE having more coverage, we've converted SHIPTYPE
into the categorical names that TYPE_NAME gave us.
From SHIPTYPE v/s LW_RATIO we can see some really sweet correlations such as Fishing ships are longer, Cargo Vessels are more "chonky". 

DWT (Deadweight Tonnage) v/s SHIPTYPE is kind of obvious

Elapsed is kind of useless since we aren't doing any real time analysis

Plotting the graph using Latitude and Longitude REALLY gives the feeling of working with marine data
It sort of looks like the map of the world!

speed v/s Geographic location! (also the range of color goes from 0-99.9% being 0 to 210)
the reason is that even if max is 405, it's an outlier which shifts the color spectrum of graph too high 
and most of fast ships are not that visible because the top one is TOO fast

using seaborn to look at the confusion matrix of the correlation of the numerical elements with each other
we see that heading and course relate, DWT and width, lenth have good relation, width and length also have good relation
this is where we start to understand what can be used as features to make predictions

Anomaly Detection Attempt (using z-scores)
tried creating anomaly scores for SPEED, HC_DIFF, and ROT to find unusual behavior
SPEED_ZSCORE worked okay, properly normalized with mean=0 std=1, max z-score was 5.83 (6 deviations away)
but HC_DIFF was pretty useless as an anomaly indicator:
-> mean is only 8.63 degrees
-> 50% of ships have HC_DIFF = 0 (median = 0) 
-> 75% have HC_DIFF â‰¤ 1 degree
-> from earlier we saw that large HC_DIFF values were mostly legitimate anyway (ships turning, drifting from wind/waves)
ROT_ZSCORE was even MORE useless:
-> 50% of values are 0, 75% are STILL 0
-> most ships just aren't turning at all if they're on course
-> only 2744 out of 10141 ships even have ROT data (27% coverage)
-> z-score is heavily skewed because of this

anomaly scoring with z-scores didn't really work here
HC_DIFF and ROT being zero or near-zero is just normal ship operation, not anomalies
the distributions are too skewed for z-scores to capture meaningful deviations
better to use domain-specific rules like "moving but anchored" or "stationary but underway" 
those are ACTUAL operational anomalies, not just statistical outliers


---

Okay, from the goals we had decided, we did the visualization part of density and heatmap
for ML, classification according to movement patterns is POSSIBLE, if we collectively use all other features wisely 
to predict the SHIPTYPE, gotta explore that. 
but anomaly and suspicious movement pattern is not possible since through analysing correlations between data, 
we can see it's not that great

No. Instead of building a toy classification project which has no potential real world value. I can just end this Data Science project on the fact that I found some good insights, 

The main problem is we only have ONE snapshot of ship data. One moment in time. Even if it's 10,000 ships, it's not so usable for anything. To get the real time data we would have to query the 

BUT, I just figured out how to bypass the rate limit of the API calls
Basically we can now have REAL TIME data of 10,000 ships. 
THIS OPENS UP POSSIBILITY FOR A LOT OF THINGS

There's one issue that we need to explore first, SHIP_ID is something that we can use to track a ship's data OVER TIME
So if we scrape data from two instances over an hour, and based on how many SHIP_ID are common between the two datasets, we can infer if the data is convertible to a time series one. 
This would take time. 

data1 & data2: 1424
data1 & data3: 1410
data1 & data4: 1410
data2 & data3: 2515
data2 & data4: 2515
data3 & data4: 9202

After two days of scraping now we have over 6 different moments of time and after using AI to do the analysis faster on those which I haven't gone over particularly because it would take a lot more time I used claude sonnet 4.5 To read the output of those cells and then give me an analysis on what exactly we can do with the data and how we can create some sort of machine learning application on it with the end goal in mind Of creating a dashboard that displays multiple things such as detecting ships that are behaving erratically, Flagging potential smuggling, Detecting drift and distress. We can also automatically discover ports, Track port congestion through the number of ships that are clustered, Detect dark zones. 

Now the main thing is trying to find ideas on what we can do that deploy right now and then automatically get better over time as more and more data is scraped and stored till a month's has been gather to train on and then it's a matter of optimization. 

There was a slight issue with script.py, because it wasn't headless we couldn't run it on VPS, but now I fixed it, just had to add some parameters and user-agent and whatever (AI did it). 

The idea has started to form!
We will create a full stack website with backend being a complete ML pipeline from data scraping to preprocessing to prediction and classification to storage to api endpoint to frontend to display on dashboard. 
We are gonna create the craziest frontend dashboard with all ships, their type etc metadata that's directly scraped + we will display the historical trajectories and whatnot. 

Now comes the cool but HARD part.
I need to learn about machine learning algorithms that we would implement specifically to be adaptive with more data. The aim is to have a 30 day rolling data where we will scrape every day 24 times and will have 720 "snapshots" per month, and we would only apply our algorithms on the entire month. As in on day 31 we would drop day 1. 

the backend is scrape -> restructure -> store -> perform predictions -> store -> rest api -> frontend -> displayed as a 3d globe with ships on the map, clickable n stuff, filters to see only anomaly or only ports (congestion data) or illegal stuff or ship in danger or click ship to see it's timeline throughout last 30 days 

the algorithms may or maynot depending on how much computation is needed run in the same time or take run every 24 hours, only the 24 hour snapshot will be displayed on the frontend, i.e. our frontend would not display real time, it would display the last run ML pipeline's predictions. We'll see how it plays out in the scalability thing. But basically at max level we are going to play with 10 million entries of data. 

the ML algorithms that we would apply in order to do all sorts of predictions and classifications, they would be deployable NOW but would get better as more and more data is scraped and stored in our database. 

In total we would be playing with around 10 million entries of data when this is deployed and at max. 

I will actually just paste the entire AI's response for my question asking a list of all the concepts that I need to learn in order to understand everything to be able to implement the entire project. Now, conceptually I WILL try to understand every single thing but to ship in a week I would still be using abstract libraries that would do the code for me. I hope for a XGBoost algorithm to just be imported from some library sklearn or something and I do XGBoost.train(data) and tune hyperparameters with my understanding. That is enough I think. 

# ALGORITHMS
DBSCAN 
K-Means Clustering
HDBSCAN
Isolation Forest
One-Class SVM
LOF
Statistical Methods (these are like, using metadata to calculate specific features that you find the "top 10" or something of or evaluate against a threshold and that would detect on its own, not that crazy but if it WORKS then it will reduce computations immensely and that is KEY in MLOps, always start with small blocks)
LSTM
GRU
XGBoost
Time Series Fundamentals (sliding window, train validation test split for time series, autocorrelation)

The Fundamentals is very important to understand the bigger picture of what we're playing with here. 

Haversine Distance Formula
Circular Statistics
Rolling window Statistics
Lag features

Model Evaluation -> unsupervised learning metrics, anomaly detection metrics 

--- 

THE BIGGEST insight I've gotten of feature engineering is FIRST recognizing the limitations of data that is just a snapshot in time, there is not much to be done. Next thing is we gather time series data so that we can measure behaviour over time. Whilst reading how we are going to do that I came across this 

# Create features for ML
def calculate_features(ship_history):
    df['SPEED_CHANGE'] = df.groupby('SHIP_ID')['SPEED'].diff()
    df['COURSE_STD'] = df.groupby('SHIP_ID')['COURSE'].rolling(6).std()
    df['DISTANCE_TRAVELED'] = haversine(df['LAT'].shift(), df['LON'].shift(),
                                         df['LAT'], df['LON'])
    df['TIME_STATIONARY'] = (df['SPEED'] < 2).groupby('SHIP_ID').cumsum()

BASICALLY Raw data (lat/lon/speed) isn't enough for ML
Derived features = capture behavior patterns
COURSE_STD = how erratic the ship is (key for anomalies)

More importantly, we are now UTILIZING THE TIME DATA IN ORDER TO CREATE DATA THAT GIVES THE NEW INSIGHT THAT's "OVERTIME"

So that's how we turn the static moments in time data and engineer features across time that are USEFUL.

Okay, the biggest problem I'm currently facing is that my debit card is prepaid and to claim the 200$ of digital ocean credits to deploy this on a server is kinda... takes a credit card. Which means... till that's sorted we can't really deploy a script or anything. 
BUT let's control what we have, I'm going to try and build everything in a docker container offline, hopefully will be able to deploy it, otherwise eh whatever. 
