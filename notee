graph TB
    subgraph "ğŸŒ EXTERNAL WORLD"
        MT[MarineTraffic.com<br/>Ship Data Source]
        USER[User's Browser]
    end

    subgraph "â˜ï¸ DIGITAL OCEAN DROPLET (Your VPS Server)"
        
        subgraph "ğŸ³ Docker Container 1: SCRAPER"
            SCRAPER[scraper.py<br/>---<br/>Selenium headless<br/>Runs every 1 hour]
            SCHEDULER1[APScheduler<br/>---<br/>Triggers scraper.py<br/>every hour]
        end

        subgraph "ğŸ³ Docker Container 2: DATABASE"
            DB[(PostgreSQL<br/>---<br/>Tables:<br/>- ships_current<br/>- ships_historical<br/>- ports_discovered<br/>- anomalies<br/>- predictions)]
        end

        subgraph "ğŸ³ Docker Container 3: ML PIPELINE"
            MLSCHED[APScheduler<br/>---<br/>Daily: 12:00 AM]
            
            ML1[port_clustering.py<br/>---<br/>DBSCAN algorithm<br/>Discovers ports]
            ML2[anomaly_detection.py<br/>---<br/>Isolation Forest<br/>Flags weird ships]
            ML3[trajectory_prediction.py<br/>---<br/>LSTM model<br/>Predicts routes]
            
            MLSCHED -->|Triggers| ML1
            ML1 -->|Then runs| ML2
            ML2 -->|Then runs| ML3
        end

        subgraph "ğŸ³ Docker Container 4: API SERVER"
            API[main.py<br/>FastAPI<br/>---<br/>REST Endpoints:<br/>GET /api/ships/current<br/>GET /api/ships/:id/history<br/>GET /api/ports<br/>GET /api/anomalies<br/>GET /api/predictions/:id]
        end

        subgraph "ğŸ³ Docker Container 5: FRONTEND"
            NEXT[Next.js App<br/>---<br/>3D Globe Mapbox<br/>Dashboard<br/>Charts]
        end

        NGINX[Nginx Reverse Proxy<br/>---<br/>Routes traffic:<br/>/ â†’ Frontend<br/>/api â†’ Backend]
    end

    %% Data Flow
    MT -->|HTTP GET| SCRAPER
    SCRAPER -->|INSERT INTO<br/>ships_current,<br/>ships_historical| DB
    
    DB -->|READ data| ML1
    ML1 -->|WRITE TO<br/>ports_discovered| DB
    ML2 -->|READ ships_historical| DB
    ML2 -->|WRITE TO<br/>anomalies| DB
    ML3 -->|READ ships_historical| DB
    ML3 -->|WRITE TO<br/>predictions| DB
    
    DB -->|SQL queries| API
    
    API -->|JSON response| NGINX
    NEXT -->|HTTP request| NGINX
    
    USER -->|HTTPS<br/>maritimeintel.me| NGINX
    NGINX -->|HTML/JS/CSS| USER

    classDef scraper fill:#ff6b6b,stroke:#c92a2a,color:#fff
    classDef db fill:#4dabf7,stroke:#1971c2,color:#fff
    classDef ml fill:#51cf66,stroke:#2f9e44,color:#fff
    classDef api fill:#ffd43b,stroke:#f08c00,color:#000
    classDef frontend fill:#845ef7,stroke:#5f3dc4,color:#fff
    classDef external fill:#868e96,stroke:#495057,color:#fff
    
    class SCRAPER,SCHEDULER1 scraper
    class DB db
    class ML1,ML2,ML3,MLSCHED ml
    class API api
    class NEXT,NGINX frontend
    class MT,USER external

    maritime-intelligence/
â”‚
â”œâ”€â”€ ğŸ“ backend/                          # â† ALL Python code here
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile                    # Docker setup for backend
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt              # Python dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ scraper/                      # â† SCRAPER (Container 1)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ scraper.py                # Selenium script (scrapes MarineTraffic)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ scheduler.py              # APScheduler (runs scraper every hour)
â”‚   â”‚   â””â”€â”€ ğŸ“„ db_writer.py              # Writes scraped data to PostgreSQL
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ ml/                           # â† ML PIPELINE (Container 3)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ml_scheduler.py           # APScheduler (runs ML daily at midnight)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ port_clustering.py        # DBSCAN algorithm
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ anomaly_detection.py      # Isolation Forest
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ trajectory_prediction.py  # LSTM model
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ feature_engineering.py    # Calculate COURSE_STD, HC_DIFF, etc.
â”‚   â”‚   â””â”€â”€ ğŸ“ models/                   # Saved ML models
â”‚   â”‚       â”œâ”€â”€ lstm_model.h5            # Trained LSTM
â”‚   â”‚       â””â”€â”€ isolation_forest.pkl     # Trained Isolation Forest
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ api/                          # â† REST API (Container 4)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ main.py                   # FastAPI app (entry point)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ routes.py                 # API endpoints (/api/ships, /api/ports...)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py                 # Pydantic models (request/response schemas)
â”‚   â”‚   â””â”€â”€ ğŸ“„ database.py               # SQLAlchemy connection to PostgreSQL
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ utils/                        # â† Shared utilities
â”‚       â”œâ”€â”€ ğŸ“„ haversine.py              # Distance calculation
â”‚       â”œâ”€â”€ ğŸ“„ config.py                 # Environment variables (DB credentials)
â”‚       â””â”€â”€ ğŸ“„ logger.py                 # Logging setup
â”‚
â”œâ”€â”€ ğŸ“ frontend/                         # â† Next.js app (Container 5)
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚   â”œâ”€â”€ ğŸ“„ package.json
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ app/                          # Next.js 13+ app directory
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ page.tsx                  # Homepage (3D globe)
â”‚   â”‚   â””â”€â”€ ğŸ“„ layout.tsx                # Root layout
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ components/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ShipGlobe.tsx             # Mapbox 3D globe component
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ShipCard.tsx              # Ship info popup
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ Dashboard.tsx             # Stats dashboard
â”‚   â”‚   â””â”€â”€ ğŸ“„ TrajectoryLine.tsx        # Ship historical path
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ lib/
â”‚       â””â”€â”€ ğŸ“„ api.ts                    # Fetch functions (calls backend API)
â”‚
â”œâ”€â”€ ğŸ“ database/                         # â† Database setup (Container 2)
â”‚   â”œâ”€â”€ ğŸ“„ init.sql                      # SQL schema (creates tables on first run)
â”‚   â””â”€â”€ ğŸ“„ docker-compose.yml            # PostgreSQL container config
â”‚
â”œâ”€â”€ ğŸ“ nginx/                            # â† Reverse proxy config
â”‚   â””â”€â”€ ğŸ“„ nginx.conf                    # Routes / â†’ frontend, /api â†’ backend
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml                # Orchestrates all 5 containers
â”œâ”€â”€ ğŸ“„ .env                              # Environment variables (DB password, API keys)
â””â”€â”€ ğŸ“„ README.md                         # Setup instructions


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” WHAT CODE RUNS WHERE & WHEN:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTAINER 1: SCRAPER (backend/scraper/)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHEN:     Every 1 hour (via APScheduler)                                    â”‚
â”‚ DOES:     Scrapes MarineTraffic â†’ Saves to PostgreSQL                       â”‚
â”‚ FILES:    scraper.py, scheduler.py, db_writer.py                            â”‚
â”‚ ENTRY:    python scheduler.py (runs forever, triggers scraper hourly)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTAINER 2: DATABASE (PostgreSQL)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHEN:     Always running                                                    â”‚
â”‚ DOES:     Stores all data (scraped ships, ML predictions, ports)            â”‚
â”‚ FILES:    init.sql (creates tables on first run)                            â”‚
â”‚ ENTRY:    docker run postgres:15 (managed by Docker)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTAINER 3: ML PIPELINE (backend/ml/)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHEN:     Every day at 12:00 AM (via APScheduler)                           â”‚
â”‚ DOES:     Reads DB â†’ Runs ML â†’ Saves predictions back to DB                 â”‚
â”‚ FILES:    ml_scheduler.py, port_clustering.py, anomaly_detection.py,        â”‚
â”‚           trajectory_prediction.py, feature_engineering.py                   â”‚
â”‚ ENTRY:    python ml_scheduler.py (runs forever, triggers ML daily)          â”‚
â”‚ FLOW:     1. Feature engineering (calculate COURSE_STD, etc.)               â”‚
â”‚           2. Port clustering (DBSCAN)                                        â”‚
â”‚           3. Anomaly detection (Isolation Forest)                            â”‚
â”‚           4. Trajectory prediction (LSTM)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTAINER 4: API SERVER (backend/api/)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHEN:     Always running (listens for HTTP requests)                        â”‚
â”‚ DOES:     Receives requests from frontend â†’ Queries DB â†’ Returns JSON       â”‚
â”‚ FILES:    main.py, routes.py, models.py, database.py                        â”‚
â”‚ ENTRY:    uvicorn main:app --host 0.0.0.0 --port 8000                       â”‚
â”‚ ENDPOINTS:                                                                   â”‚
â”‚   GET /api/ships/current          â†’ Returns all 10k ships right now         â”‚
â”‚   GET /api/ships/:id/history      â†’ Returns 30-day trajectory of one ship   â”‚
â”‚   GET /api/ports                  â†’ Returns discovered ports                â”‚
â”‚   GET /api/anomalies              â†’ Returns flagged ships                   â”‚
â”‚   GET /api/predictions/:id        â†’ Returns predicted route for one ship    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTAINER 5: FRONTEND (frontend/)                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHEN:     Always running (serves website)                                   â”‚
â”‚ DOES:     Displays 3D globe â†’ Fetches data from API â†’ Renders UI            â”‚
â”‚ FILES:    All .tsx files in frontend/                                       â”‚
â”‚ ENTRY:    npm run dev (development) or npm run build + npm start (prod)     â”‚
â”‚ FLOW:     User opens browser â†’ Next.js loads â†’ Calls /api/ships/current â†’   â”‚
â”‚           Displays ships on Mapbox globe                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NGINX REVERSE PROXY (nginx/)                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHEN:     Always running                                                    â”‚
â”‚ DOES:     Routes incoming traffic to correct container                      â”‚
â”‚ ROUTES:   https://maritimeintel.me/        â†’ Container 5 (Frontend)         â”‚
â”‚           https://maritimeintel.me/api/... â†’ Container 4 (API)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ DEPLOYMENT CHECKLIST:

1. SSH into DigitalOcean Droplet
2. Clone your repo: git clone https://github.com/you/maritime-intelligence
3. Create .env file (DB password, Mapbox token)
4. Run: docker-compose up -d
5. Containers start automatically:
   - PostgreSQL creates tables (from init.sql)
   - Scraper starts scraping every hour
   - API server starts listening on port 8000
   - ML pipeline waits until midnight to run
   - Frontend builds and starts on port 3000
   - Nginx routes traffic from port 80

Done! Visit your-droplet-ip:80 in browser.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â±ï¸ TIMELINE OF WHAT RUNS WHEN:

12:00 AM (Midnight)
  â””â”€ ML Pipeline runs (takes ~20 minutes)
     1. Feature engineering (5 min)
     2. Port clustering (2 min)
     3. Anomaly detection (3 min)
     4. Trajectory prediction (10 min)
     â””â”€ Results saved to DB

Every hour (1 AM, 2 AM, 3 AM...)
  â””â”€ Scraper runs (takes ~2 minutes)
     1. Scrapes MarineTraffic
     2. Parses 10k ships
     3. Saves to ships_current, ships_historical
     â””â”€ DB updated

All day (24/7)
  â””â”€ API Server + Frontend running
     - User visits website
     - Frontend calls /api/ships/current
     - API queries DB
     - Returns JSON
     - Frontend displays on globe


sequenceDiagram
    participant MT as MarineTraffic.com
    participant SC as Scraper (scheduler.py)
    participant DB as PostgreSQL
    participant ML as ML Pipeline (ml_scheduler.py)
    participant API as FastAPI (main.py)
    participant FE as Frontend (Next.js)
    participant User as User Browser

    Note over SC,ML: â•â•â• HOURLY SCRAPE â•â•â•
    
    SC->>SC: APScheduler triggers<br/>(every 1 hour)
    SC->>MT: HTTP GET /getData/get_data_json_4
    MT-->>SC: JSON (10,000 ships)
    SC->>SC: Parse JSON into DataFrame
    SC->>DB: INSERT INTO ships_current
    SC->>DB: INSERT INTO ships_historical
    DB-->>SC: âœ“ Data saved
    
    Note over ML,API: â•â•â• DAILY ML PIPELINE (Midnight) â•â•â•
    
    ML->>ML: APScheduler triggers<br/>(12:00 AM daily)
    ML->>DB: SELECT * FROM ships_historical<br/>WHERE timestamp > NOW() - 7 days
    DB-->>ML: 168 hours of data per ship
    
    ML->>ML: feature_engineering.py<br/>Calculate COURSE_STD, HC_DIFF, etc.
    
    ML->>ML: port_clustering.py (DBSCAN)<br/>Find ships with SPEED < 2
    ML->>DB: INSERT INTO ports_discovered
    
    ML->>ML: anomaly_detection.py<br/>(Isolation Forest)
    ML->>DB: INSERT INTO anomalies
    
    ML->>ML: trajectory_prediction.py<br/>(LSTM)
    ML->>DB: INSERT INTO predictions
    
    DB-->>ML: âœ“ ML results saved
    
    Note over User,FE: â•â•â• USER VISITS WEBSITE â•â•â•
    
    User->>FE: Opens https://maritimeintel.me
    FE->>FE: Next.js loads page.tsx
    
    FE->>API: GET /api/ships/current
    API->>DB: SELECT * FROM ships_current
    DB-->>API: 10,000 ships
    API-->>FE: JSON response
    FE->>FE: Mapbox renders ships on globe
    
    User->>FE: Clicks ship #12345
    FE->>API: GET /api/ships/12345/history
    API->>DB: SELECT * FROM ships_historical<br/>WHERE ship_id = 12345
    DB-->>API: 30 days of positions
    API-->>FE: JSON response
    FE->>FE: Draw trajectory line on map
    
    FE->>API: GET /api/predictions/12345
    API->>DB: SELECT * FROM predictions<br/>WHERE ship_id = 12345
    DB-->>API: Predicted route (next 6 hours)
    API-->>FE: JSON response
    FE->>FE: Draw dotted prediction line
    
    FE->>API: GET /api/ports
    API->>DB: SELECT * FROM ports_discovered
    DB-->>API: ~100 port locations
    API-->>FE: JSON response
    FE->>FE: Draw port circles on map
    
    FE->>API: GET /api/anomalies
    API->>DB: SELECT * FROM anomalies
    DB-->>API: Flagged ships
    API-->>FE: JSON response
    FE->>FE: Mark anomalous ships in red
    
    FE-->>User: Complete dashboard displayed!

graph LR
    subgraph Internet
        USER[ğŸ‘¤ User<br/>Browser]
        MT[ğŸŒ MarineTraffic]
    end

    subgraph DigitalOcean["â˜ï¸ DigitalOcean Droplet (your-ip-address)"]
        
        NGINX["ğŸ”€ Nginx<br/>Port 80/443<br/>---<br/>Reverse Proxy"]
        
        subgraph Docker Network
            SC["ğŸ“¡ Scraper<br/>No exposed port<br/>---<br/>Runs: scheduler.py"]
            
            DB["ğŸ—„ï¸ PostgreSQL<br/>Port 5432<br/>---<br/>Database"]
            
            ML["ğŸ¤– ML Pipeline<br/>No exposed port<br/>---<br/>Runs: ml_scheduler.py"]
            
            API["âš¡ FastAPI<br/>Port 8000<br/>---<br/>REST API"]
            
            FE["ğŸ¨ Frontend<br/>Port 3000<br/>---<br/>Next.js"]
        end
    end

    %% External connections
    MT -->|"GET /getData<br/>(every hour)"| SC
    USER -->|"HTTPS<br/>maritimeintel.me"| NGINX
    
    %% Internal connections
    NGINX -->|"/ â†’ proxy_pass"| FE
    NGINX -->|"/api â†’ proxy_pass"| API
    
    SC -->|"SQL INSERT<br/>(every hour)"| DB
    ML -->|"SQL SELECT/INSERT<br/>(daily midnight)"| DB
    API -->|"SQL SELECT<br/>(on request)"| DB
    FE -->|"HTTP GET /api/*"| API

    %% Styling
    classDef external fill:#e9ecef,stroke:#495057
    classDef proxy fill:#ffd43b,stroke:#f08c00,color:#000
    classDef container fill:#4dabf7,stroke:#1971c2,color:#fff
    
    class USER,MT external
    class NGINX proxy
    class SC,DB,ML,API,FE container

ğŸ¯ KEY TAKEAWAYS (Connecting the Pieces)
1. Physical Location - Everything runs on ONE DigitalOcean Droplet
Your Droplet IP: 123.45.67.89
â”œâ”€â”€ 5 Docker containers running simultaneously
â”œâ”€â”€ Nginx routing traffic from port 80
â””â”€â”€ All containers talk to each other via Docker network
```

### **2. Code Organization - Two Main Folders**
```
backend/   â† All Python code (scraper + ML + API)
frontend/  â† All Next.js code (you already know this)
3. Three Python "Programs" Running Independently
Program 1: Scraper (runs every hour)
bashpython backend/scraper/scheduler.py  # Runs forever, triggers scraper.py hourly
Program 2: ML Pipeline (runs daily at midnight)
bashpython backend/ml/ml_scheduler.py  # Runs forever, triggers ML scripts daily
Program 3: API Server (runs 24/7, handles requests)
bashuvicorn backend/api/main:app  # Runs forever, listens on port 8000
```

### **4. How They Connect**
```
Scraper â†’ PostgreSQL â† ML Pipeline
                â†“
            API Server â† Frontend â† User
All communication happens through PostgreSQL:

Scraper writes new ship data
ML reads that data, runs algorithms, writes predictions
API reads everything and serves it as JSON
Frontend fetches JSON and displays it

5. No Direct Communication Between Containers
âŒ Scraper does NOT call ML Pipeline
âŒ ML Pipeline does NOT call API
âŒ API does NOT call Scraper
âœ… They all just read/write to the shared database
âœ… They run on their own schedules independently
âœ… Database is the "source of truth"
6. Why This Architecture Works

Decoupled - If scraper crashes, API still works (serves old data)
Scalable - Can move ML to separate server if it gets slow
Simple - Each container has ONE job
Debuggable - Check logs for each container separately


in a fashion wehre I can just do something like create a keep notes task list that i can do in sprints wehre theres a clear end point etc, I wnat you to give me a breakdown of this idea in 7 sprints. I'm going to first create the entire thing (i already have the scraping script and the analysis, i wanna start building everything else) 